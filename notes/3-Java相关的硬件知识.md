# Java相关的硬件知识

CPU和内存是计算机的核心

## CPU内的各个组件

PC -> Program Counter 程序计数器 （记录当前指令在内存中的地址，找下一条指令的时候并不是简单的加一，要看指令的长度）  
Registers -> 暂时存储CPU计算需要用到的数据，CPU的一等公民。现在有几十上百个，写汇编要知道每一个寄存器是干什么的。现在是64位寄存器
             JVM的本地变量表就相当于是JVM的寄存器了，但是速度没法跟真实的寄存器相比  
ALU -> Arithmetic & Logic Unit 运算单元。具体怎么运算呢？mov ax 2，mov bx 3，然后add，就自动去ax和bx中找两个加数，然后运算，
       最后输出到一个寄存器中，再把其值写到内存中的某个地址  
CU -> Control Unit 控制单元。对终端信号等做控制的。  
MMU -> Memory Management Unit 内存管理单元。最早的内存管理都是操作系统软件实现的，现在都是硬件+操作系统实现的，后面讲到再说  
cache: L1,L2(在每个core之内都有)，L3（同一个CPU的各个core共享），速度依次下降  
  
什么是超线程？特别简单：一个ALU对应着多组寄存器（Register）和程序计数器（PC）。如果只有一组，就得在线程切换执行的时候，把当前Register和PC
的数据复制到缓存，然后把另一个线程的响应的数据复制进Register和PC，再执行，如此往复。如果有两套Register和PC，ALU只需要转向另一套就可以实现
线程之间的切换执行了，不用来回复制，提高了效率。
  
GPU为什么比CPU更适合做机器学习？因为前者是专用芯片而后者是通用芯片，前者的结构更适合于做单纯的AI计算。现在华为、阿里等公司还在设计，专门的
AI芯片，比如NPU。这就更加专业化了，AI计算更加快速。  

CPU与外设靠系统总线、IO bridge和IO总线联系起来，一旦外设有输入，就会产生CPU中断。

## 各种存储器的层次结构

各个级别的存储结构其速度排序如下图：
![存储器层次结构](images/cache-1.jpg)

高速缓存结构：  
![高速缓存结构](images/cache-2.png)
  
CPU访问寄存器、各级缓存和内存的速度：  
存储器|访问时间
---- | ---
Registers| <1ns
L1 Cache|约1ns
L2 Cache|约3ns
L3 Cache|约15ns
Main Memory|约80ns

### 缓存的读取方式
按块读取，由于程序局部性原理（当用到某些数据的时候，它旁边的也会被用到），按块，而不是按位或者字节读取效率更高，充分发挥总线CPU针脚等一次性
读取更多数据的能力。按块读取的原理不管是从硬盘往内存读，还是从内存往缓存读，都是这个原理。从硬盘读数据到内存的时候，用的是DMA（Direct 
Memory Access），而不是走CPU。按块读取的时候，块的大小有的是自定义的，有的是被写死了的。  

缓存是一行行读的，读一个数据的时候，先去L1找，再去L2找，然后L3、内存，然后把内存读出的数据依次缓存到L3、L2、L1。工程测试，这么做最合适。
如果x、y位于同一个缓存行，Core1需要用到x，Core2需要用到y，则x、y以及他们所在的这个缓存行整体都会被读到两个Core的各级缓存。如果同一个缓存
行的数据数据需要同步，则要内存一致性协议（MESI）或者锁总线保证。保持缓存一致性是指：一旦一个CPU改了缓存行中的一个数据，他要告诉另外的CPU，
这一行数据已经过时了，请重新再去内存里读一遍。  

#### MESI
四种状态：Modified、Exclusive、Shared、Invalid。在一个CPU里面的缓存行是Modified，修改过了，则在里一个中就是Invalid，要重新读取。文本
特别繁琐，不用看。除了MESI之外，还有很多缓存一致性协议：MOSI、MSI、Synapse、Firefly、Dragon

#### 锁总线
有的数据一个缓存行装不下，这个时候就去锁定总线，肯定就没有数据不一致的情况了，但是跟MESI相比效率低，能用MESI的就用MESI

#### 缓存行的大小
缓存行越大，局部性空间效率越高，但是读取时间慢；缓存行越小，局部空间效率越低，但是读取时间快。取个折中值，目前多用64字节。

#### 缓存行对齐的编程方式
在Distruptor中，在long cursor的前后都加上7个long，保证他绝对不会跟其他的数据放到同一个缓存行里。加上@Contended也可以保证这个变量跟
别的变量不在同一个缓存行, 但是JVM要设置参数：`-XX:-RestrictContended`. @Contended可以根据底层的CPU缓存行的长度设定，办证变量独占一个
缓存行

### "乱序"执行

指令1先执行，去读内存并等待返回，内存和缓存速度的比大概是100 : 1, 为了提高效率，如果下一条指令2跟指令1没有关系，则会去先执行指令2，在程序
看来是指令2先执行，指令1后执行，虽然写的时候是指令1在前面。volatile关键字可以避免这种指令重排序。汇编指令new：只会申请一块空间，不会给成员
变量赋值，只有执行了invokespecial之后，才会赋初值，new的时候会在栈里面有一个引用指向new的对象，而dup命令会在栈顶复制一个引用，也指向这个
对象。这是因为后面的invokespecial会消耗一个值，把存在栈顶的这个值弹出来，调用它指向的这个对象的构造方法。astore_1是把栈顶的值弹出来，赋值
给局部变量表排在第一位的变量（位置为0的是this）。

### CPU层面如何禁止指令重排
答：内存屏障。对某一部分内存操作的时候，前后添加的屏障，屏障前后的操作不可以乱序执行屏障上面的命令全执行完了，才可以执行屏障下面的命令。
Intel底层通过lfence、sfence、mfence，分别是读（load）、写（save）和读写屏障（mixed）。当然也可以通过总线锁来实现。volatile并不是用的
这里的原语实现的，而是lock指令（锁屏障，lock addl 0x0 esp）.Lock指令是一个原子指令，如x86上的"lock..."指令是一个Full Barrier（读写
指令都不能进行重排序），执行时会锁住内存子系统(锁内存总线)来确保执行顺序，甚至跨多个CPU。Software Locks通常使用了内存屏障或原子指令来实现
变量可见性和保持程序顺序. Lock指令一执行，整个Lock前面的都的执行完了，该刷的数据刷到内存里面去，后面的才能继续执行。所以使用原语要比lock
指令效率高，s/m/lfence 原语Intel CPU有，其他的CPU不一定有，但是Lock指令很多CPU都有。所以Hotspot在实现的时候就偷懒了，并没有根据不同的
CPU来时用各自的原语，而是统一用Lock指令（周志明的书上有说）

### JSR内存屏障

#### 定义
这是Java 虚拟机要求的规范，跟硬件没有关系，是不同层面的事情。Java就要求某些指令在某些JVM的屏障前后不能重排序，所以以下是JVM的规范要求。
然后Hotspot再去写实现，可以用s/m/lfence或者Lock实现。这里只说JVM的规范的要求：
- LoadLoad屏障  
  对于这样的语句：
  ```
  Load1;
  LoadLoad屏障
  Load2
  ```
  在Load2及后续读取操作要读取的数据被访问前，保证Load1要读取的数据已被读取完毕
- StoreStore屏障  
  对于这样的语句：
  ```
  Store1;
  StoreStore屏障
  Store2
  ```
  在Store2及后续写入操作执行前，保证Store1的写入操作对其他处理器可见。
- LoadStore屏障  
  对于这样的语句：
  ```
  Load1;
  LoadStore屏障
  Store2
  ```
  在Store2及后续写入操作被刷出前，保证Load1要读取的数据已被读取完毕
- StoreLoad屏障  
  对于这样的语句：
  ```
  Store1;
  StoreLoad屏障
  Load2
  ```
  在Load2及后续所有读取操作执行前，保证Store1的写入对所有处理器可见
  
#### volatile 的实现细节
JVM层面（背过，好像是ali问到了volatile在JVM层级的实现，很保守，具体还是用Lock指令实现的）：
```
StoreStore屏障
volatile写操作
StoreLoad屏障
```
volatile写不能早于上面的写操作，而下面的的读取，必须要等这里的写操作完成之后才可以读。这就保证了对下面代码的可见性，而且还不会有重排序
```
LoadLoad屏障
volatile读操作
LoadStore屏障
```
上面的Load跟volatile Load不可互换，下面的写操作要等volatile读完了才可以写，也保证了对下面代码的可见性，而且还不会有重排序。  

volatile变量t如果指向了对象，对整个t所指向的对象的改动都会前后加内存屏障，以保证不会重排序.  

### 禁止乱序

- CPU层面上，Intel -> 原语：s/m/lfence或者锁总线
- JVM层面上8个`happens before`原则和4个内存屏障（LL、LS、SL、SS）
`as if serial`: 不管如何重排序，单线程执行结果不变。很多请求发过来之后要求先来后到顺序执行：用SingleThreadPool，队列做成有界队列，
然后设置拒绝策略

### 合并写技术（选看, 就是好玩）
ALU和内存之间有L1-3 cache，ALU和L1之间还有一级缓存，一个buffer，只有4字节（写到L1的3-4ns对CPU来说还是太长），叫WC buffer，Write
Combining Buffer，这个buffer被CPU写满之后，一次性刷到L2缓存中。这块就别跟面试官聊了，对方八成儿不懂，认为你是在装懂。

### NUMA
Non Uniformed Memory Access，非同一内存访问。多个CPU插槽附近都有各自就近分布的内存，就近访问要比访问别的地方的内存速度快的多，优先访问、
往里面分配对象。UMA不容易扩展，CPU个数增多之后，内存争用加剧，性能都损耗在了争抢内存地址上面，工业上4颗CPU比较合适。ZGC就是NUMA aware的。

## 计算机启动 （不重要）

- BIOS 或者 UEFI 加电自检，加载bootloader到内存。bootloader是引导程序，管着让你选择到底启动那个操作系统。Bootloader在硬盘上的位置也是
写死的，
- 有些东西还是没写死，可以选择。CMOS芯片存储一些可以配置的信息，比如密码和启动顺序，靠一节电池加电保持记忆。
- 选定了OS之后，CPU跳到OS启动的那个点上，开始执行OS程序，剩下的所有的事情，都是由OS老大管理了。

## 操作系统
### 鸿蒙微内核操作系统
- 针对5G + IoT
- 全场景：手机、PC、平板、车辆、智能穿戴、家居设备...
- 弹性部署
- 开源
- 方舟编译器
- 混合内核Linux + LiteOS

### 什么是操作系统
一边管硬件，一边管应用软件。《Linux内核设计与实现》
- 内核：面向底层管理硬件。管理文件系统、应用、进程调度、中断处理、设备驱动、内存管理、CPU调度。宏内核有以上各种功能；微内核只管进程调度。
       其他的功能（比如读硬盘）甚至可以在其他的机器上。这就是为什么鸿蒙是面向5G和IoT，可插拔式的。所以HW并不是要干掉安卓或者微软。宏内核
       效率高，但是微内核有弹性，符合万物互联的特点。鸿蒙的各种功能的技术底层核心就是`微内核`. 外核：做一个专门和browser匹配的OS，类比
       阿里的session/request base垃圾回收器（他们还有多租户GC）
- 外壳：跟应用程序打交道
- VMM 功能特别强的计算机虚拟化出很多OS，充分利用资源。中间就会有一层 Virtual Machine Monitor监控

### 内核基础概念
Dos中的应用程序可以直接写硬件相关的内容。当代OS中通过划分用户态和内核台来区分哪些操作可以由用户来做，哪些不可以。CPU - CPL -> 0 1 2 3
Linux用了ring0 （内核态、OS） 和 ring3 （三环外，用户态、应用程序），（其他的没用到）来区分用户态和内核态。所有需要内核态操作的事情都要通过OS
kernel，用户态和内核态来回切换。所以内存也分了内核空间和用户空间，后者不能修改前者，OS越来越健壮了。内核执行的操作 -> 200 多个系统调用：
sendfile、read、write、pthread、fork。JVM对OS来说是个普通程序，它运行在用户态

#### 堆外内存
JVM从硬盘或者网络度过来的数据是先由内核读到内核空间，然后再复制给JVM的堆，如果JVM能直接访问堆外内存，则它的效率就会高很多，参见Netty、epoll
和虚引用。直接内存严格讲跟对外内存不是一回事儿，但是很多文章里就是指一回事儿




### 答疑
Java的同一个线程，在多个CPU下，是同一个CPU完成他还是多个核配合完成？多核共同完成。长连接归根结底全是TCP，其实也可以用UDP模拟TCP